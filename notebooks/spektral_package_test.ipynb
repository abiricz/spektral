{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from spektral.datasets import citation\n",
    "import spektral\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spektral.layers import GraphConv\n",
    "from spektral import utils\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source:\n",
    "https://danielegrattarola.github.io/spektral/getting-started/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset\n",
      "Pre-processing node features\n"
     ]
    }
   ],
   "source": [
    "data = spektral.datasets.citation.load_data('cora')\n",
    "A, X, y_train, y_val, y_test, train_mask, val_mask, test_mask = data\n",
    "\n",
    "N = A.shape[0]\n",
    "F = X.shape[-1]\n",
    "n_classes = y_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 2708), (2708, 1433), (2708, 7), (2708, 7), (2708,), (2708,), (2708,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, X.shape, y_val.shape, y_test.shape, train_mask.shape, val_mask.shape, test_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abiricz/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/abiricz/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "X_in = Input(shape=(F, ))  # Input layer for X\n",
    "A_in = Input((N, ), sparse=True)  # Input layer for A\n",
    "\n",
    "graph_conv_1 = GraphConv(16, activation='relu')([X_in, A_in])\n",
    "dropout = Dropout(0.5)(graph_conv_1)\n",
    "graph_conv_2 = GraphConv(n_classes, activation='softmax')([dropout, A_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_conv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = utils.localpooling_filter(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2708)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_1 (GraphConv)        (None, 16)           22944       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           graph_conv_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_2 (GraphConv)        (None, 7)            119         dropout_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,063\n",
      "Trainable params: 23,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abiricz/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2708 samples, validate on 2708 samples\n",
      "Epoch 1/100\n",
      "2708/2708 [==============================] - 1s 437us/step - loss: 1.9461 - weighted_acc: 0.1929 - val_loss: 1.9457 - val_weighted_acc: 0.1260\n",
      "Epoch 2/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.9453 - weighted_acc: 0.2429 - val_loss: 1.9451 - val_weighted_acc: 0.1540\n",
      "Epoch 3/100\n",
      "2708/2708 [==============================] - 0s 12us/step - loss: 1.9444 - weighted_acc: 0.2714 - val_loss: 1.9445 - val_weighted_acc: 0.2100\n",
      "Epoch 4/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.9437 - weighted_acc: 0.2857 - val_loss: 1.9440 - val_weighted_acc: 0.2740\n",
      "Epoch 5/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.9430 - weighted_acc: 0.3429 - val_loss: 1.9435 - val_weighted_acc: 0.2900\n",
      "Epoch 6/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.9415 - weighted_acc: 0.3786 - val_loss: 1.9431 - val_weighted_acc: 0.3100\n",
      "Epoch 7/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.9405 - weighted_acc: 0.4214 - val_loss: 1.9426 - val_weighted_acc: 0.3440\n",
      "Epoch 8/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.9406 - weighted_acc: 0.4286 - val_loss: 1.9422 - val_weighted_acc: 0.3600\n",
      "Epoch 9/100\n",
      "2708/2708 [==============================] - 0s 12us/step - loss: 1.9393 - weighted_acc: 0.4357 - val_loss: 1.9418 - val_weighted_acc: 0.3500\n",
      "Epoch 10/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.9377 - weighted_acc: 0.4714 - val_loss: 1.9413 - val_weighted_acc: 0.3440\n",
      "Epoch 11/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.9368 - weighted_acc: 0.4429 - val_loss: 1.9409 - val_weighted_acc: 0.3580\n",
      "Epoch 12/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.9364 - weighted_acc: 0.3929 - val_loss: 1.9404 - val_weighted_acc: 0.3680\n",
      "Epoch 13/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.9344 - weighted_acc: 0.5286 - val_loss: 1.9399 - val_weighted_acc: 0.3860\n",
      "Epoch 14/100\n",
      "2708/2708 [==============================] - 0s 21us/step - loss: 1.9339 - weighted_acc: 0.5000 - val_loss: 1.9393 - val_weighted_acc: 0.3920\n",
      "Epoch 15/100\n",
      "2708/2708 [==============================] - 0s 16us/step - loss: 1.9321 - weighted_acc: 0.4857 - val_loss: 1.9387 - val_weighted_acc: 0.4020\n",
      "Epoch 16/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.9314 - weighted_acc: 0.4500 - val_loss: 1.9380 - val_weighted_acc: 0.4220\n",
      "Epoch 17/100\n",
      "2708/2708 [==============================] - 0s 16us/step - loss: 1.9299 - weighted_acc: 0.5071 - val_loss: 1.9373 - val_weighted_acc: 0.4360\n",
      "Epoch 18/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.9293 - weighted_acc: 0.4714 - val_loss: 1.9366 - val_weighted_acc: 0.4400\n",
      "Epoch 19/100\n",
      "2708/2708 [==============================] - 0s 14us/step - loss: 1.9285 - weighted_acc: 0.4857 - val_loss: 1.9359 - val_weighted_acc: 0.4540\n",
      "Epoch 20/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.9262 - weighted_acc: 0.5500 - val_loss: 1.9352 - val_weighted_acc: 0.4620\n",
      "Epoch 21/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.9249 - weighted_acc: 0.5071 - val_loss: 1.9345 - val_weighted_acc: 0.4780\n",
      "Epoch 22/100\n",
      "2708/2708 [==============================] - 0s 20us/step - loss: 1.9250 - weighted_acc: 0.5500 - val_loss: 1.9338 - val_weighted_acc: 0.4940\n",
      "Epoch 23/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.9215 - weighted_acc: 0.5714 - val_loss: 1.9331 - val_weighted_acc: 0.5080\n",
      "Epoch 24/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.9208 - weighted_acc: 0.6500 - val_loss: 1.9324 - val_weighted_acc: 0.5200\n",
      "Epoch 25/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.9187 - weighted_acc: 0.6214 - val_loss: 1.9317 - val_weighted_acc: 0.5220\n",
      "Epoch 26/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.9212 - weighted_acc: 0.6357 - val_loss: 1.9311 - val_weighted_acc: 0.5220\n",
      "Epoch 27/100\n",
      "2708/2708 [==============================] - 0s 18us/step - loss: 1.9169 - weighted_acc: 0.6214 - val_loss: 1.9304 - val_weighted_acc: 0.5420\n",
      "Epoch 28/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.9169 - weighted_acc: 0.6357 - val_loss: 1.9297 - val_weighted_acc: 0.5460\n",
      "Epoch 29/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.9128 - weighted_acc: 0.6857 - val_loss: 1.9291 - val_weighted_acc: 0.5560\n",
      "Epoch 30/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.9123 - weighted_acc: 0.7571 - val_loss: 1.9284 - val_weighted_acc: 0.5700\n",
      "Epoch 31/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.9115 - weighted_acc: 0.7000 - val_loss: 1.9277 - val_weighted_acc: 0.5720\n",
      "Epoch 32/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.9102 - weighted_acc: 0.7286 - val_loss: 1.9271 - val_weighted_acc: 0.5720\n",
      "Epoch 33/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.9113 - weighted_acc: 0.7071 - val_loss: 1.9264 - val_weighted_acc: 0.5720\n",
      "Epoch 34/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.9076 - weighted_acc: 0.7143 - val_loss: 1.9258 - val_weighted_acc: 0.5780\n",
      "Epoch 35/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.9055 - weighted_acc: 0.6857 - val_loss: 1.9251 - val_weighted_acc: 0.5760\n",
      "Epoch 36/100\n",
      "2708/2708 [==============================] - 0s 14us/step - loss: 1.9032 - weighted_acc: 0.7143 - val_loss: 1.9244 - val_weighted_acc: 0.5700\n",
      "Epoch 37/100\n",
      "2708/2708 [==============================] - 0s 21us/step - loss: 1.9033 - weighted_acc: 0.7500 - val_loss: 1.9237 - val_weighted_acc: 0.5620\n",
      "Epoch 38/100\n",
      "2708/2708 [==============================] - 0s 14us/step - loss: 1.9005 - weighted_acc: 0.7429 - val_loss: 1.9229 - val_weighted_acc: 0.5600\n",
      "Epoch 39/100\n",
      "2708/2708 [==============================] - 0s 14us/step - loss: 1.8986 - weighted_acc: 0.7143 - val_loss: 1.9222 - val_weighted_acc: 0.5540\n",
      "Epoch 40/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.8984 - weighted_acc: 0.7143 - val_loss: 1.9215 - val_weighted_acc: 0.5540\n",
      "Epoch 41/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.8968 - weighted_acc: 0.7143 - val_loss: 1.9208 - val_weighted_acc: 0.5560\n",
      "Epoch 42/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8962 - weighted_acc: 0.6786 - val_loss: 1.9200 - val_weighted_acc: 0.5560\n",
      "Epoch 43/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.8936 - weighted_acc: 0.7429 - val_loss: 1.9192 - val_weighted_acc: 0.5600\n",
      "Epoch 44/100\n",
      "2708/2708 [==============================] - 0s 20us/step - loss: 1.8917 - weighted_acc: 0.8071 - val_loss: 1.9183 - val_weighted_acc: 0.5700\n",
      "Epoch 45/100\n",
      "2708/2708 [==============================] - 0s 20us/step - loss: 1.8885 - weighted_acc: 0.7571 - val_loss: 1.9175 - val_weighted_acc: 0.5720\n",
      "Epoch 46/100\n",
      "2708/2708 [==============================] - 0s 18us/step - loss: 1.8853 - weighted_acc: 0.8000 - val_loss: 1.9167 - val_weighted_acc: 0.5700\n",
      "Epoch 47/100\n",
      "2708/2708 [==============================] - 0s 16us/step - loss: 1.8868 - weighted_acc: 0.7429 - val_loss: 1.9159 - val_weighted_acc: 0.5720\n",
      "Epoch 48/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.8827 - weighted_acc: 0.7286 - val_loss: 1.9151 - val_weighted_acc: 0.5800\n",
      "Epoch 49/100\n",
      "2708/2708 [==============================] - 0s 14us/step - loss: 1.8808 - weighted_acc: 0.7714 - val_loss: 1.9142 - val_weighted_acc: 0.5800\n",
      "Epoch 50/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.8818 - weighted_acc: 0.7500 - val_loss: 1.9134 - val_weighted_acc: 0.5820\n",
      "Epoch 51/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.8820 - weighted_acc: 0.7714 - val_loss: 1.9126 - val_weighted_acc: 0.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.8801 - weighted_acc: 0.7571 - val_loss: 1.9118 - val_weighted_acc: 0.5900\n",
      "Epoch 53/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.8767 - weighted_acc: 0.7857 - val_loss: 1.9109 - val_weighted_acc: 0.5900\n",
      "Epoch 54/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.8731 - weighted_acc: 0.7857 - val_loss: 1.9100 - val_weighted_acc: 0.5980\n",
      "Epoch 55/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8732 - weighted_acc: 0.7714 - val_loss: 1.9092 - val_weighted_acc: 0.5980\n",
      "Epoch 56/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.8696 - weighted_acc: 0.7643 - val_loss: 1.9083 - val_weighted_acc: 0.6000\n",
      "Epoch 57/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.8719 - weighted_acc: 0.7714 - val_loss: 1.9074 - val_weighted_acc: 0.6040\n",
      "Epoch 58/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.8673 - weighted_acc: 0.8214 - val_loss: 1.9065 - val_weighted_acc: 0.6020\n",
      "Epoch 59/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.8677 - weighted_acc: 0.7929 - val_loss: 1.9055 - val_weighted_acc: 0.6100\n",
      "Epoch 60/100\n",
      "2708/2708 [==============================] - 0s 14us/step - loss: 1.8650 - weighted_acc: 0.8286 - val_loss: 1.9045 - val_weighted_acc: 0.6120\n",
      "Epoch 61/100\n",
      "2708/2708 [==============================] - 0s 12us/step - loss: 1.8635 - weighted_acc: 0.7357 - val_loss: 1.9035 - val_weighted_acc: 0.6200\n",
      "Epoch 62/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.8569 - weighted_acc: 0.8214 - val_loss: 1.9025 - val_weighted_acc: 0.6200\n",
      "Epoch 63/100\n",
      "2708/2708 [==============================] - 0s 16us/step - loss: 1.8570 - weighted_acc: 0.7929 - val_loss: 1.9015 - val_weighted_acc: 0.6100\n",
      "Epoch 64/100\n",
      "2708/2708 [==============================] - 0s 13us/step - loss: 1.8519 - weighted_acc: 0.8000 - val_loss: 1.9004 - val_weighted_acc: 0.6100\n",
      "Epoch 65/100\n",
      "2708/2708 [==============================] - 0s 21us/step - loss: 1.8493 - weighted_acc: 0.8500 - val_loss: 1.8994 - val_weighted_acc: 0.6100\n",
      "Epoch 66/100\n",
      "2708/2708 [==============================] - 0s 20us/step - loss: 1.8494 - weighted_acc: 0.8286 - val_loss: 1.8984 - val_weighted_acc: 0.6100\n",
      "Epoch 67/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.8507 - weighted_acc: 0.7500 - val_loss: 1.8974 - val_weighted_acc: 0.6080\n",
      "Epoch 68/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.8464 - weighted_acc: 0.8357 - val_loss: 1.8963 - val_weighted_acc: 0.6080\n",
      "Epoch 69/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.8450 - weighted_acc: 0.8571 - val_loss: 1.8953 - val_weighted_acc: 0.6080\n",
      "Epoch 70/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.8453 - weighted_acc: 0.8000 - val_loss: 1.8943 - val_weighted_acc: 0.6080\n",
      "Epoch 71/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8430 - weighted_acc: 0.8071 - val_loss: 1.8934 - val_weighted_acc: 0.6080\n",
      "Epoch 72/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8421 - weighted_acc: 0.7857 - val_loss: 1.8925 - val_weighted_acc: 0.6060\n",
      "Epoch 73/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.8347 - weighted_acc: 0.7929 - val_loss: 1.8916 - val_weighted_acc: 0.6100\n",
      "Epoch 74/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8370 - weighted_acc: 0.7929 - val_loss: 1.8906 - val_weighted_acc: 0.6080\n",
      "Epoch 75/100\n",
      "2708/2708 [==============================] - 0s 21us/step - loss: 1.8371 - weighted_acc: 0.8071 - val_loss: 1.8897 - val_weighted_acc: 0.6120\n",
      "Epoch 76/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.8343 - weighted_acc: 0.8500 - val_loss: 1.8888 - val_weighted_acc: 0.6160\n",
      "Epoch 77/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.8290 - weighted_acc: 0.8357 - val_loss: 1.8878 - val_weighted_acc: 0.6120\n",
      "Epoch 78/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.8318 - weighted_acc: 0.7929 - val_loss: 1.8869 - val_weighted_acc: 0.6140\n",
      "Epoch 79/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.8260 - weighted_acc: 0.7786 - val_loss: 1.8859 - val_weighted_acc: 0.6160\n",
      "Epoch 80/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.8244 - weighted_acc: 0.7929 - val_loss: 1.8849 - val_weighted_acc: 0.6140\n",
      "Epoch 81/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.8245 - weighted_acc: 0.8000 - val_loss: 1.8838 - val_weighted_acc: 0.6160\n",
      "Epoch 82/100\n",
      "2708/2708 [==============================] - 0s 24us/step - loss: 1.8199 - weighted_acc: 0.8000 - val_loss: 1.8827 - val_weighted_acc: 0.6160\n",
      "Epoch 83/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8136 - weighted_acc: 0.8143 - val_loss: 1.8816 - val_weighted_acc: 0.6180\n",
      "Epoch 84/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.8109 - weighted_acc: 0.7929 - val_loss: 1.8804 - val_weighted_acc: 0.6220\n",
      "Epoch 85/100\n",
      "2708/2708 [==============================] - 0s 17us/step - loss: 1.8094 - weighted_acc: 0.8071 - val_loss: 1.8793 - val_weighted_acc: 0.6180\n",
      "Epoch 86/100\n",
      "2708/2708 [==============================] - 0s 16us/step - loss: 1.8071 - weighted_acc: 0.7857 - val_loss: 1.8782 - val_weighted_acc: 0.6180\n",
      "Epoch 87/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.8053 - weighted_acc: 0.8286 - val_loss: 1.8771 - val_weighted_acc: 0.6200\n",
      "Epoch 88/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8123 - weighted_acc: 0.8214 - val_loss: 1.8760 - val_weighted_acc: 0.6220\n",
      "Epoch 89/100\n",
      "2708/2708 [==============================] - 0s 19us/step - loss: 1.8066 - weighted_acc: 0.7786 - val_loss: 1.8748 - val_weighted_acc: 0.6280\n",
      "Epoch 90/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.8057 - weighted_acc: 0.8143 - val_loss: 1.8736 - val_weighted_acc: 0.6280\n",
      "Epoch 91/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.7989 - weighted_acc: 0.7929 - val_loss: 1.8724 - val_weighted_acc: 0.6260\n",
      "Epoch 92/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.7936 - weighted_acc: 0.8143 - val_loss: 1.8712 - val_weighted_acc: 0.6320\n",
      "Epoch 93/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.7919 - weighted_acc: 0.8286 - val_loss: 1.8701 - val_weighted_acc: 0.6360\n",
      "Epoch 94/100\n",
      "2708/2708 [==============================] - 0s 18us/step - loss: 1.7955 - weighted_acc: 0.8071 - val_loss: 1.8689 - val_weighted_acc: 0.6380\n",
      "Epoch 95/100\n",
      "2708/2708 [==============================] - 0s 16us/step - loss: 1.7798 - weighted_acc: 0.8357 - val_loss: 1.8677 - val_weighted_acc: 0.6320\n",
      "Epoch 96/100\n",
      "2708/2708 [==============================] - 0s 15us/step - loss: 1.7896 - weighted_acc: 0.8000 - val_loss: 1.8665 - val_weighted_acc: 0.6300\n",
      "Epoch 97/100\n",
      "2708/2708 [==============================] - 0s 22us/step - loss: 1.7904 - weighted_acc: 0.7929 - val_loss: 1.8653 - val_weighted_acc: 0.6300\n",
      "Epoch 98/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.7941 - weighted_acc: 0.8071 - val_loss: 1.8641 - val_weighted_acc: 0.6360\n",
      "Epoch 99/100\n",
      "2708/2708 [==============================] - 0s 20us/step - loss: 1.7798 - weighted_acc: 0.8000 - val_loss: 1.8629 - val_weighted_acc: 0.6360\n",
      "Epoch 100/100\n",
      "2708/2708 [==============================] - 0s 23us/step - loss: 1.7871 - weighted_acc: 0.8214 - val_loss: 1.8617 - val_weighted_acc: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f976f654390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "validation_data = ([X, A], y_val, val_mask)\n",
    "model.fit([X, A],\n",
    "          y_train,\n",
    "          sample_weight=train_mask,\n",
    "          epochs=100,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False)  # Shuffling data means shuffling the whole graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708/2708 [==============================] - 0s 5us/step\n",
      "Done.\n",
      "Test loss: 1.8558011054992676\n",
      "Test accuracy: 0.6709999442100525\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "eval_results = model.evaluate([X, A],\n",
    "                              y_test,\n",
    "                              sample_weight=test_mask,\n",
    "                              batch_size=N)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test accuracy: {}'.format(*eval_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader:\n",
    "\n",
    "https://github.com/danielegrattarola/spektral/blob/master/spektral/datasets/qm9.py\n",
    "\n",
    "https://github.com/danielegrattarola/spektral/blob/master/examples/regression_molecules.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from spektral.datasets import qm9\n",
    "from spektral.layers import EdgeConditionedConv, GlobalAttentionPool\n",
    "from spektral.utils import label_to_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QM9 dataset.\n",
      "Reading SDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133885/133885 [00:30<00:00, 4352.05it/s]\n"
     ]
    }
   ],
   "source": [
    "adj, nf, ef, y = qm9.load_data(return_type='numpy',\n",
    "                               nf_keys='atomic_num',\n",
    "                               ef_keys='type',\n",
    "                               self_loops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  157.7118    ,   157.70997   ,   157.70699   , ...,\n",
       "         -398.64329001,  -401.01464652,  -372.47177215],\n",
       "       [  293.60975   ,   293.54111   ,   191.39397   , ...,\n",
       "         -278.62027109,  -280.3992591 ,  -259.33880205],\n",
       "       [  799.58812   ,   437.90386   ,   282.94545   , ...,\n",
       "         -213.97429391,  -215.15965841,  -201.40717117],\n",
       "       ...,\n",
       "       [    3.67118   ,     2.14314   ,     1.89501   , ...,\n",
       "        -1678.83004849, -1688.3129645 , -1549.14339097],\n",
       "       [    3.52845   ,     2.15131   ,     1.86582   , ...,\n",
       "        -1807.21085978, -1817.2867718 , -1670.34989187],\n",
       "       [    3.64015   ,     2.21764   ,     1.93793   , ...,\n",
       "        -1614.4551552 , -1623.3450752 , -1492.24714994]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_array = y[['cv']].values # Heat capacity at 298.15K\n",
    "y_array = y.iloc[:, 1:].values\n",
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "uniq_nf = np.unique(nf)\n",
    "nf = label_to_one_hot(nf, uniq_nf)\n",
    "uniq_ef = np.unique(ef)\n",
    "ef = label_to_one_hot(ef, uniq_ef)\n",
    "y_array_scaled = StandardScaler().fit_transform(y_array).reshape(-1, y_array.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "adj_train, adj_test, \\\n",
    "nf_train, nf_test,   \\\n",
    "ef_train, ef_test,   \\\n",
    "y_train, y_test = train_test_split(adj, nf, ef, y_array_scaled, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = nf.shape[-2]          # Number of nodes in the graphs\n",
    "F = nf.shape[-1]          # Node features dimensionality\n",
    "S = ef.shape[-1]          # Edge features dimensionality\n",
    "n_out = y_array.shape[-1]       # Dimensionality of the target\n",
    "learning_rate = 1e-3      # Learning rate for SGD\n",
    "epochs = 25               # Number of training epochs\n",
    "batch_size = 64           # Batch size\n",
    "es_patience = 5           # Patience fot early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "nf_in = Input(shape=(N, F))\n",
    "adj_in = Input(shape=(N, N))\n",
    "ef_in = Input(shape=(N, N, S))\n",
    "\n",
    "gc1 = EdgeConditionedConv(32, activation='relu')([nf_in, adj_in, ef_in])\n",
    "gc2 = EdgeConditionedConv(64, activation='relu')([gc1, adj_in, ef_in])\n",
    "pool = GlobalAttentionPool(128)(gc2)\n",
    "dense1 = Dense(128, activation='relu')(pool)\n",
    "\n",
    "output = Dense(n_out)(dense1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 29, 6)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 29, 29)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 29, 29, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_conditioned_conv_3 (EdgeCo (None, 29, 32)       992         input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "edge_conditioned_conv_4 (EdgeCo (None, 29, 64)       10304       edge_conditioned_conv_3[0][0]    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_attention_pool_2 (Global (None, 128)          16640       edge_conditioned_conv_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       global_attention_pool_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 19)           2451        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,899\n",
      "Trainable params: 46,899\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Model(inputs=[nf_in, adj_in, ef_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=es_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120496, 29, 6), (120496, 29, 29), (120496, 29, 29, 4), (120496, 19))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf_train.shape, adj_train.shape, ef_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108446 samples, validate on 12050 samples\n",
      "Epoch 1/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.2481 - val_loss: 0.0896\n",
      "Epoch 2/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1913 - val_loss: 0.0790\n",
      "Epoch 3/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1824 - val_loss: 0.1104\n",
      "Epoch 4/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1766 - val_loss: 0.0802\n",
      "Epoch 5/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1719 - val_loss: 0.0719\n",
      "Epoch 6/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1709 - val_loss: 0.0992\n",
      "Epoch 7/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1764 - val_loss: 0.0850\n",
      "Epoch 8/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1427 - val_loss: 0.0723\n",
      "Epoch 9/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1578 - val_loss: 0.0707\n",
      "Epoch 10/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1424 - val_loss: 0.0693\n",
      "Epoch 11/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1324 - val_loss: 0.0856\n",
      "Epoch 12/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1419 - val_loss: 0.0730\n",
      "Epoch 13/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1400 - val_loss: 0.0618\n",
      "Epoch 14/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1453 - val_loss: 0.0942\n",
      "Epoch 15/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1321 - val_loss: 0.0655\n",
      "Epoch 16/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1413 - val_loss: 0.0568\n",
      "Epoch 17/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1307 - val_loss: 0.0796\n",
      "Epoch 18/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1399 - val_loss: 0.0672\n",
      "Epoch 19/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1293 - val_loss: 0.1221\n",
      "Epoch 20/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1344 - val_loss: 0.0568\n",
      "Epoch 21/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1308 - val_loss: 0.0995\n",
      "Epoch 22/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1171 - val_loss: 0.0557\n",
      "Epoch 23/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1227 - val_loss: 0.0569\n",
      "Epoch 24/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1179 - val_loss: 0.0589\n",
      "Epoch 25/25\n",
      "108446/108446 [==============================] - 109s 1ms/step - loss: 0.1195 - val_loss: 0.0500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda6224a0f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit([nf_train, adj_train, ef_train],\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1,\n",
    "          epochs=epochs,\n",
    "          callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model.\n",
      "13389/13389 [==============================] - 7s 508us/step\n",
      "Done.\n",
      "Test loss: 0.3709455115873798\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model.evaluate([nf_test, adj_test, ef_test],\n",
    "                              y_test,\n",
    "                              batch_size=batch_size)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}'.format(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_id</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>mu</th>\n",
       "      <th>alpha</th>\n",
       "      <th>homo</th>\n",
       "      <th>lumo</th>\n",
       "      <th>gap</th>\n",
       "      <th>r2</th>\n",
       "      <th>zpve</th>\n",
       "      <th>u0</th>\n",
       "      <th>u298</th>\n",
       "      <th>h298</th>\n",
       "      <th>g298</th>\n",
       "      <th>cv</th>\n",
       "      <th>u0_atom</th>\n",
       "      <th>u298_atom</th>\n",
       "      <th>h298_atom</th>\n",
       "      <th>g298_atom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gdb_1</td>\n",
       "      <td>157.71180</td>\n",
       "      <td>157.709970</td>\n",
       "      <td>157.706990</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13.21</td>\n",
       "      <td>-0.3877</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>35.3641</td>\n",
       "      <td>0.044749</td>\n",
       "      <td>-40.478930</td>\n",
       "      <td>-40.476062</td>\n",
       "      <td>-40.475117</td>\n",
       "      <td>-40.498597</td>\n",
       "      <td>6.469</td>\n",
       "      <td>-395.999595</td>\n",
       "      <td>-398.643290</td>\n",
       "      <td>-401.014647</td>\n",
       "      <td>-372.471772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gdb_2</td>\n",
       "      <td>293.60975</td>\n",
       "      <td>293.541110</td>\n",
       "      <td>191.393970</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>9.46</td>\n",
       "      <td>-0.2570</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.3399</td>\n",
       "      <td>26.1563</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>-56.525887</td>\n",
       "      <td>-56.523026</td>\n",
       "      <td>-56.522082</td>\n",
       "      <td>-56.544961</td>\n",
       "      <td>6.316</td>\n",
       "      <td>-276.861363</td>\n",
       "      <td>-278.620271</td>\n",
       "      <td>-280.399259</td>\n",
       "      <td>-259.338802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gdb_3</td>\n",
       "      <td>799.58812</td>\n",
       "      <td>437.903860</td>\n",
       "      <td>282.945450</td>\n",
       "      <td>1.8511</td>\n",
       "      <td>6.31</td>\n",
       "      <td>-0.2928</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>0.3615</td>\n",
       "      <td>19.0002</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>-76.404702</td>\n",
       "      <td>-76.401867</td>\n",
       "      <td>-76.400922</td>\n",
       "      <td>-76.422349</td>\n",
       "      <td>6.002</td>\n",
       "      <td>-213.087624</td>\n",
       "      <td>-213.974294</td>\n",
       "      <td>-215.159658</td>\n",
       "      <td>-201.407171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gdb_4</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.28</td>\n",
       "      <td>-0.2845</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.3351</td>\n",
       "      <td>59.5248</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>-77.308427</td>\n",
       "      <td>-77.305527</td>\n",
       "      <td>-77.304583</td>\n",
       "      <td>-77.327429</td>\n",
       "      <td>8.574</td>\n",
       "      <td>-385.501997</td>\n",
       "      <td>-387.237686</td>\n",
       "      <td>-389.016047</td>\n",
       "      <td>-365.800724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gdb_5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>2.8937</td>\n",
       "      <td>12.99</td>\n",
       "      <td>-0.3604</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>48.7476</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>-93.411888</td>\n",
       "      <td>-93.409370</td>\n",
       "      <td>-93.408425</td>\n",
       "      <td>-93.431246</td>\n",
       "      <td>6.278</td>\n",
       "      <td>-301.820534</td>\n",
       "      <td>-302.906752</td>\n",
       "      <td>-304.091489</td>\n",
       "      <td>-288.720028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mol_id          A           B           C      mu  alpha    homo    lumo  \\\n",
       "0  gdb_1  157.71180  157.709970  157.706990  0.0000  13.21 -0.3877  0.1171   \n",
       "1  gdb_2  293.60975  293.541110  191.393970  1.6256   9.46 -0.2570  0.0829   \n",
       "2  gdb_3  799.58812  437.903860  282.945450  1.8511   6.31 -0.2928  0.0687   \n",
       "3  gdb_4    0.00000   35.610036   35.610036  0.0000  16.28 -0.2845  0.0506   \n",
       "4  gdb_5    0.00000   44.593883   44.593883  2.8937  12.99 -0.3604  0.0191   \n",
       "\n",
       "      gap       r2      zpve         u0       u298       h298       g298  \\\n",
       "0  0.5048  35.3641  0.044749 -40.478930 -40.476062 -40.475117 -40.498597   \n",
       "1  0.3399  26.1563  0.034358 -56.525887 -56.523026 -56.522082 -56.544961   \n",
       "2  0.3615  19.0002  0.021375 -76.404702 -76.401867 -76.400922 -76.422349   \n",
       "3  0.3351  59.5248  0.026841 -77.308427 -77.305527 -77.304583 -77.327429   \n",
       "4  0.3796  48.7476  0.016601 -93.411888 -93.409370 -93.408425 -93.431246   \n",
       "\n",
       "      cv     u0_atom   u298_atom   h298_atom   g298_atom  \n",
       "0  6.469 -395.999595 -398.643290 -401.014647 -372.471772  \n",
       "1  6.316 -276.861363 -278.620271 -280.399259 -259.338802  \n",
       "2  6.002 -213.087624 -213.974294 -215.159658 -201.407171  \n",
       "3  8.574 -385.501997 -387.237686 -389.016047 -365.800724  \n",
       "4  6.278 -301.820534 -302.906752 -304.091489 -288.720028  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13389, 19), (13389, 19))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "preds = model.predict([nf_test, adj_test, ef_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Actual')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(preds[:,-5], y_test[:,-5], alpha=0.3)\n",
    "plt.plot(range(-6, 6), range(-6, 6))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "#plt.savefig('pred_v_true.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1651348761879315"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( np.sum( (preds[:,-5] - y_test[:,-5])**2 ) / preds.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11946759377970118"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( np.sum( (preds - y_test)**2 ) / preds.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08124633166564237"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( np.sum( (preds[:,-4] - y_test[:,-4])**2 ) / preds.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08613860009925375"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( np.sum( (preds[:,-3] - y_test[:,-3])**2 ) / preds.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08031927784362079"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( np.sum( (preds[:,-2] - y_test[:,-2])**2 ) / preds.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07381313047415901"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt( np.sum( (preds[:,-1] - y_test[:,-1])**2 ) / preds.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
